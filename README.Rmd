---
output: github_document
editor_options: 
  chunk_output_type: console
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-"
)
options(width = 99)
```

# campfin <img src="man/figures/logo.png" align="right" width="120" />

[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/campfin)](https://cran.r-project.org/package=campfin)

## Overview

The `campfin` package was created to facilitate the work being done by 
[The Accountability Project][01], a tool created by 
[The Investigative Reporting Workshop][02]. The Accountability Project curates,
cleans and indexes public data to give journalists, researchers and others a
simple way to search across otherwise siloed records. The data focuses on
people, organizations and locations. This package was created specifically to
helo with state-level **camp**aign **fin**ance data.

[01]: https://www.publicaccountability.org/ "tap"
[02]: https://investigativereportingworkshop.org/ "irw"

## Installation

The package is not on CRAN and must be installed from GitHub.

```{r install, eval=TRUE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load_current_gh("kiernann/campfin")
pacman::p_load(tidyverse, knitr, stringdist, zipcode)
```

## Normalize

The most important functions are the in the `normal_*()` family. These functions
take geographic data and return [normalized text][03] that is more searchable.
They are largely wrappers around the [`stringr` package][07].

* `normal_zip()` takes [ZIP Codes][04] and returns a 5 digit character string
* `normal_state()` takes US states and returns a [2 digit abbreviation][05]
* `normal_address()` takes a _street_ address and reduces inconsistencies
* `normal_city()` takes cities and reduces inconsistencies

[03]: https://en.wikipedia.org/wiki/Text_normalization "text_normal"
[04]: https://en.wikipedia.org/wiki/ZIP_Code "zip_code"
[05]: https://en.wikipedia.org/wiki/List_of_U.S._state_abbreviations "state_abbs"
[06]: https://github.com/OpenRefine/OpenRefine/wiki/Clustering-In-Depth "open_refine"
[07]: https://github.com/tidyverse/stringr "stringr"

The `vt_contribs` built-in example data set contains many of the problems these
functions are designed to help with.

```{r view_messy}
vt_contribs
```

What are some of the problems we can see in this data?

* The `date` column is not parsed as an R date.
* There is one negative `amount` value and another that's zero.
* One record is missing both the contributor's `first` and `last` name.
* In `address` we see:
    * Inconsisten capitalization,
    * A mix of full and abbreviatied suffixes,
    * Invalid text in the place of `NA`,
    * Uneccesary and inconsistent punctuation,
    * Excess trailing white space,
    * Excess internal white space,
    * Hyphens instead of spaces,
    * Repeating character strings used as `NA`.
* In `city` we see many of the same problems, plus:
    * Geographic abbreviations,
    * Repeated `state` information,
    * Misspellings,
    * Colloquial city abbreviations.
* In `state` we see a mix of full and abbreviated state names.
* In `zip`,
    * Repeated digits used for `NA`
    * Uneccesary and inconsistent [ZIP+4][zip4] usage
    * Leading zeroes [dropped by Excel][excel] or some other program

[zip4]: https://en.wikipedia.org/wiki/ZIP_Code#ZIP+4 "zip4"
[excel]: https://support.office.com/en-us/article/display-numbers-as-postal-codes-61b55c9f-6fe3-4e54-96ca-9e85c38a5a1d "excel"

We can use the `normal_*()` family functions to clean this data into a much more
consitent format. First, we'll read the file with `readr::read_csv()`. Most
American campaign finance data uses the mm/dd/yyyy format. We can parse this as
a proper R date with the `col_date_usa` function, which is a shortcut for
`readr::col_date(format = "%m/%d/%Y")`.

```{r read_messy}
vt <- read_csv(
  file = "data-raw/vt_contribs.csv",
  trim_ws = FALSE,
  col_types = cols(
    amount = col_number(),
    date = col_date_usa()
  )
)
```

Next, we should try to normalize our data as much as possible. We can use
some simple counting functions and built in vectors to check our progress.

```{r normal_address}
vt <- vt %>% 
  mutate(
    address = normal_address(
      address = address,
      add_abbs = usps_street,
      na = invalid_city,
      na_rep = TRUE
    ),
    city = normal_city(
      city = city,
      geo_abbs = usps_city,
      st_abbs = "VT",
      na = invalid_city,
      na_rep = TRUE
    ),
    state = normal_state(
      state = state,
      abbreviate = TRUE,
      na_rep = TRUE,
      valid = valid_state
    ),
    zip = normal_zip(
      zip = zip,
      na_rep = TRUE
    )
  )
```

We can see how these functions and our built in data was used to normalize the
geographic contributor data and remove anything that didn't present real
information. This format is much more explorable and searchable.

```{r show_normal, echo=FALSE}
print(vt)
```

However, we can see now every problem has been solved. Most troublesome are the
city names. 

```{r bad_city, echo=FALSE}
vt %>%
  select(1, 8:10) %>% 
  drop_na(city) %>% 
  mutate(valid = city %in% valid_city) %>%
  filter(!valid)
```

There is some built in data and a regular process we use to fix as
much of these problems as possible. Checking against the _expected_ city for
a given ZIP code is a fast, easy, and confident way to fix incorrect `city`
values. The `is_abbrev()` and `stringdist::stringdist()` functions are great for
this.

```{r swap_city}
vt <- vt %>%
  rename(city_raw = city) %>% 
  left_join(zipcodes) %>% 
  rename(city_match = city) %>% 
  mutate(
    match_dist = stringdist(city_raw, city_match),
    match_abb = is_abbrev(city_raw, city_match),
    city = if_else(match_abb | match_dist == 1, city_match, city_raw)
  ) %>% 
  select(-city_raw, -city_match, -match_dist, -match_abb)
```

```{r show_swap, echo=FALSE}
vt %>%
  select(1, 8:10) %>% 
  drop_na(city) %>% 
  mutate(
    valid = city %in% valid_city & state %in% valid_state & zip %in% valid_zip
    )
```

Once our data is as normal as we can confidently make it, we can begind to
explore. First, we'll explore the data for missing values with `flag_na`, which
takes a [tidyselct][tidyselect] number of key columns to check (or something
like `dplyr::everything()`).

```{r flag_na}
flag_na(vt, last)
```

Next, we'll want to check for duplicate rows using `flag_dupes`, which takes
the same kind of arguments. Here, we can ignore the supposedly unique `id`
variable. It's possible for a person to make the same contribution on the same
date, but we should flag them nonetheless.

```{r flag_dupes}
flag_dupes(vt, -id)
```

[tidyselect]: https://github.com/r-lib/tidyselect "tidyselect"

## Data

The campfin package contains a number of built in data frames and strings used to help wrangle
campaign finance data.

```{r data}
cat(data(package = "campfin")$results[, "Item"], sep = "\n")
```

The `geo` [tibble][08] is a normalized version of the `zipcodes` data frame from the 
[`zipcodes`][09] R package, which itself is a version of the [CivicSpace US ZIP Code Database][10].

The `valid_city`, `valid_state`, and `valid_zip` are the unique, sorted columns of thr `geo` data
frame.

[08]: https://tibble.tidyverse.org/ "tibble"
[09]: https://cran.r-project.org/web/packages/zipcode/ "zip_pkg"
[10]: https://boutell.com/zipcodes/ "civic_space"

```{r geo_df, collapse=TRUE, warning=FALSE, message=FALSE, error=FALSE}
# zipcode version
data("zipcode")
sample_n(zipcode, 5)
class(zipcode)

# campfin version
sample_n(zipcodes, 5)
class(zipcodes)

# more US states than the built in state.abb
setdiff(valid_state, state.abb)
```

The `na_city` vector contains common invalid city names, which can be passed to `normal_city()`.

```{r na_city}
sample(invalid_city, 5)
```

The `usps_*` data frames can be used with `normal_*()` to
expand the [official USPS abbreviations](https://pe.usps.com/text/pub28/28apc_002.htm).

```{r usps}
sample_n(usps_city, 5)
sample_n(usps_state, 5)
sample_n(usps_street, 5)
```

The `rx_zip` and `rx_state` character strings are useful regular expressions for extracting data
from a single string address, which can then be passed to `normal_zip()` and `normal_state()`.

```{r print_rx, collapse=TRUE}
print(rx_zip)
print(rx_state)
```

```{r rx_strings, collapse=TRUE}
white_house <- "1600 Pennsylvania Ave NW, Washington, DC 20500-0003"
str_extract(white_house, pattern = rx_zip)
str_extract(white_house, pattern = rx_state)
```

Work is being done to incorperate regular expressions for addresses and city names, although
the immense possibility for variation makes these elements harder to generalize.
